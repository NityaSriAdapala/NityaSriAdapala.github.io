<!DOCTYPE html>
<html>
<head>
    <title>Project 5</title>
    <style>
        h1 {
            font-family: Georgia, sans-serif;
            margin: 20px;
            text-align: center;
        }
        h2 {
            font-family: Georgia, sans-serif;
            margin: 20px;
            text-align: center;
        }
        h3 {
            font-family: Georgia, sans-serif;
            margin: 20px;
            text-align: center;
        }
        p {
            font-family: Georgia, sans-serif;
            margin: 20px;
            padding: 10px;
        }
        ul {
            font-family: Georgia, sans-serif;
            margin: 20px;
            padding: 10px;
        }
        .image-container {
            display: flex;
            justify-content: center;
            margin: 10px;
            align-items: center;
        }
        .image-container img {
            margin: 20 auto;
            box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.1);
            height: 300px;
            width: auto;
            max-width: 100%;
            align-items: center;
            display: block;
            margin: 30px;
        }
        .image-caption {
            margin: 5px 0;
            text-align: center;
        }
        .scaled-image {
            width: 80%; 
            height: auto;  
        }
        hr.rounded {
          border-top: 2px solid #000;
          border-radius: 5px;
        }
        
    </style>
</head>
<body>
    <h1> Project 5 </h1>

    <h2> NITYA SRI ADAPALA </h2>

    <hr class="rounded">
    <h2> Part A: The power of diffusion models! </h2>
    <hr class="rounded">

    <hr class="rounded">
    <h3> Part 0: Setup </h3>
    <hr class="rounded">

    <p> For this project, I am going to use the DeepFloyd IF diffusion model. DeepFloyd is a two stage model trained by Stability AI. The first stage produces images of size 64 x 64
        and the second stage takes the outputs of the first stage and generates images of size 256 x 256. DeepFloyd was trained as a text-to-image model, which takes text prompts as 
        input and outputs images that are aligned with the text. Because the text encoder is very large, and barely fits on a free tier Colab GPU, I precomputed a couple of text 
        embeddings to try. In the notebook, I instantiate DeepFloyd's stage_1 and stage_2 objects used for generation, as well as several text prompts for sample generation. Below are
        three text prompts and their corresponding images. For the first one, I set the number of inference steps to 20. The images are detailed and very colourful but the definition 
        of the images are not too sharp - it is still a little blurry. For the second one, I increased the number of inference steps to 40. The images' definition increased as the 
        outlines of the shapes and figures are cleaner. The man wearing the hat seems more realistic than the previous set of images whereas the rocket ship seems more like a 2D cartoon.
        For the third one, I decreased the number of inference steps to 5. The quality of the images decreased by a lot and the colours are very dull and muted. The images are not as
        clear as the previous sets of images and there is some speckling across the images which makes it harder to see the key features in the image. The objects in the image seem 
        to be represented more abstractly and less realistically. Therefore, increasing the number of inference steps produces more detailed, bright and realistic images whereas 
        decreasing the number of inference steps produces more abstract, dull, and less detailed images. I am using the seed = 180. 
        </p>

    <div class="image-container">
        <div>
            <img src="media/134.png" alt="num_inference_steps = 20">
            <div class="image-caption">num_inference_steps = 20</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/135.png" alt="num_inference_steps = 40">
            <div class="image-caption">num_inference_steps = 40</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/136.png" alt="num_inference_steps = 5">
            <div class="image-caption">num_inference_steps = 5</div>
        </div>
    </div>

    <hr class="rounded">
    <h3> Part 1: Sampling Loops </h3>
    <hr class="rounded">
    
    <hr class="rounded">
    <h3> 1.1 Implementing the Forward Process </h3>
    <hr class="rounded">

    <p> A key part of diffusion is the forward process, which takes a clean image and adds noise to it. In this part, I write a function to implement this. Given a clean image, 
        we get a noisy image at timestep t by sampling from a Gaussian. Note that the forward process is not just adding noise, we also scale the image.</p>

    <div class="image-container">
        <div>
            <img src="media/01.png" alt="Noisy Campanile at t=250">
            <div class="image-caption">Noisy Campanile at t=250</div>
        </div>
        <div>
            <img src="media/02.png" alt="Noisy Campanile at t=500">
            <div class="image-caption">Noisy Campanile at t=500</div>
        </div>
        <div>
            <img src="media/03.png" alt="Noisy Campanile at t=750">
            <div class="image-caption">Noisy Campanile at t=750</div>
        </div>
    </div>

    <hr class="rounded">
    <h3> 1.2 Classical Denoising </h3>
    <hr class="rounded">

    <p> In this section, I take noisy images for timesteps [250, 500, 750], but use Gaussian blur filtering to try to remove the noise. Getting good results was 
        quite difficult, if not impossible.</p>

    <div class="image-container">
        <div>
            <img src="media/01.png" alt="Noisy Campanile at t=250">
            <div class="image-caption">Noisy Campanile at t=250</div>
        </div>
        <div>
            <img src="media/02.png" alt="Noisy Campanile at t=500">
            <div class="image-caption">Noisy Campanile at t=500</div>
        </div>
        <div>
            <img src="media/03.png" alt="Noisy Campanile at t=750">
            <div class="image-caption">Noisy Campanile at t=750</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/04.png" alt="Gaussian Blur Denoising at t=250">
            <div class="image-caption">Gaussian Blur Denoising at t=250</div>
        </div>
        <div>
            <img src="media/05.png" alt="Gaussian Blur Denoising at t=500">
            <div class="image-caption">Gaussian Blur Denoising at t=500</div>
        </div>
        <div>
            <img src="media/06.png" alt="Gaussian Blur Denoising at t=750">
            <div class="image-caption">Gaussian Blur Denoising at t=750</div>
        </div>
    </div>

    <hr class="rounded">
    <h3> 1.3 One-Step Denoising </h3>
    <hr class="rounded">

    <p> Now, I use a pretrained diffusion model to denoise. The actual denoiser is stage_1.unet, which is a UNet that has already been trained on a very, very large dataset of 
        pairs of images. We can use it to recover Gaussian noise from the image. Then, we can remove this noise to recover something close to the original image. 
        Note: this UNet is conditioned on the amount of Gaussian noise by taking timestep t as additional input.</p>

    <div class="image-container">
        <div>
            <img src="media/07.png" alt="Gaussian Blur Denoising at t=250">
            <div class="image-caption">Gaussian Blur Denoising at t=250</div>
        </div>
    </div>

    <hr class="rounded">
    <h3> 1.4 Iterative Denoising </h3>
    <hr class="rounded">

    <p> In the previous part, the denoising UNet does a much better job of projecting the image onto the natural image manifold, but it does get worse as you add more noise. This 
        makes sense, as the problem is much harder with more noise! However, diffusion models are designed to denoise iteratively, which is what I implement in this section. </p>

    <div class="image-container">
        <div>
            <img src="media/08.png" alt="Noisy Campanile at t=690">
            <div class="image-caption">Noisy Campanile at t=690</div>
        </div>
        <div>
            <img src="media/09.png" alt="Noisy Campanile at t=540">
            <div class="image-caption">Noisy Campanile at t=540</div>
        </div>
        <div>
            <img src="media/10.png" alt="Noisy Campanile at t=390">
            <div class="image-caption">Noisy Campanile at t=390</div>
        </div>
        <div>
            <img src="media/11.png" alt="Noisy Campanile at t=240">
            <div class="image-caption">Noisy Campanile at t=240</div>
        </div>
        <div>
            <img src="media/12.png" alt="Noisy Campanile at t=90">
            <div class="image-caption">Noisy Campanile at t=90</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/13.png" alt="Iteratively Denoised Campanile">
            <div class="image-caption">Iteratively Denoised Campanile</div>
        </div>
        <div>
            <img src="media/14.png" alt="One-Step Denoised Campanile">
            <div class="image-caption">One-Step Denoised Campanile</div>
        </div>
        <div>
            <img src="media/15.png" alt="Gaussian Blurred Campanile">
            <div class="image-caption">Gaussian Blurred Campanile</div>
        </div>
    </div>

    <hr class="rounded">
    <h3> 1.5 Diffusion Model Sampling </h3>
    <hr class="rounded">

    <p> Another thing we can do with the iterative_denoise function is to generate images from scratch. We can do this by setting i_start = 0 and passing in random noise. 
        This effectively denoises pure noise. </p>

    <div class="image-container">
        <div>
            <img src="media/16.png" alt="Sample 1">
            <div class="image-caption">Sample 1</div>
        </div>
        <div>
            <img src="media/17.png" alt="Sample 2">
            <div class="image-caption">Sample 2</div>
        </div>
        <div>
            <img src="media/18.png" alt="Sample 3">
            <div class="image-caption">Sample 3</div>
        </div>
        <div>
            <img src="media/19.png" alt="Sample 4">
            <div class="image-caption">Sample 4</div>
        </div>
        <div>
            <img src="media/20.png" alt="Sample 5">
            <div class="image-caption">Sample 5</div>
        </div>
    </div>

    <hr class="rounded">
    <h3> 1.6 Classifier-Free Guidance (CFG) </h3>
    <hr class="rounded">

    <p> In order to greatly improve image quality (at the expense of image diversity), we can use a technique called Classifier-Free Guidance.In CFG, we compute both a conditional 
        and an unconditional noise estimate, which we use with gamma (strength of the CFG) to calculate our new noise estimate. To get an unconditional noise estimate, we can just 
        pass an empty prompt embedding to the diffusion model. When gamma > 1, we get much higher quality images. </p>

    <div class="image-container">
        <div>
            <img src="media/21.png" alt="Sample 1 with CFG">
            <div class="image-caption">Sample 1 with CFG</div>
        </div>
        <div>
            <img src="media/22.png" alt="Sample 2 with CFG">
            <div class="image-caption">Sample 2 with CFG</div>
        </div>
        <div>
            <img src="media/23.png" alt="Sample 3 with CFG">
            <div class="image-caption">Sample 3 with CFG</div>
        </div>
        <div>
            <img src="media/24.png" alt="Sample 4 with CFG">
            <div class="image-caption">Sample 4 with CFG</div>
        </div>
        <div>
            <img src="media/25.png" alt="Sample 5 with CFG">
            <div class="image-caption">Sample 5 with CFG</div>
        </div>
    </div>

    <hr class="rounded">
    <h3> 1.7 Image-to-image Translation </h3>
    <hr class="rounded">

    <p> In part 1.4, we take a real image, add noise to it, and then denoise. This effectively allows us to make edits to existing images. The more noise we add, the larger the 
        edit will be. This works because in order to denoise an image, the diffusion model must to some extent "hallucinate" new things -- the model has to be "creative." Another 
        way to think about it is that the denoising process "forces" a noisy image back onto the manifold of natural images. Here, I am going to take the original test image, 
        noise it a little, and force it back onto the image manifold without any conditioning. Effectively, I am going to get an image that is similar to the test image (with 
        a low-enough noise level). This follows the SDEdit algorithm.</p>

    <div class="image-container">
        <div>
            <img src="media/26.png" alt="SDEdit with i_start=1">
            <div class="image-caption">SDEdit with i_start=1</div>
        </div>
        <div>
            <img src="media/27.png" alt="SDEdit with i_start=3">
            <div class="image-caption">SDEdit with i_start=3</div>
        </div>
        <div>
            <img src="media/28.png" alt="SDEdit with i_start=5">
            <div class="image-caption">SDEdit with i_start=5</div>
        </div>
    </div>
    
    <div class="image-container">
        <div>
            <img src="media/29.png" alt="SDEdit with i_start=7">
            <div class="image-caption">SDEdit with i_start=7</div>
        </div>
        <div>
            <img src="media/30.png" alt="SDEdit with i_start=10">
            <div class="image-caption">SDEdit with i_start=10</div>
        </div>
        <div>
            <img src="media/31.png" alt="SDEdit with i_start=20">
            <div class="image-caption">SDEdit with i_start=20</div>
        </div>
        <div>
            <img src="media/81.png" alt="Original Image">
            <div class="image-caption">Original Image</div>
        </div>
    </div>

    <hr class="rounded">

    <div class="image-container">
        <div>
            <img src="media/32.png" alt="SDEdit with i_start=1">
            <div class="image-caption">SDEdit with i_start=1</div>
        </div>
        <div>
            <img src="media/33.png" alt="SDEdit with i_start=3">
            <div class="image-caption">SDEdit with i_start=3</div>
        </div>
        <div>
            <img src="media/34.png" alt="SDEdit with i_start=5">
            <div class="image-caption">SDEdit with i_start=5</div>
        </div>
    </div>
    
    <div class="image-container">
        <div>
            <img src="media/35.png" alt="SDEdit with i_start=7">
            <div class="image-caption">SDEdit with i_start=7</div>
        </div>
        <div>
            <img src="media/36.png" alt="SDEdit with i_start=10">
            <div class="image-caption">SDEdit with i_start=10</div>
        </div>
        <div>
            <img src="media/37.png" alt="SDEdit with i_start=20">
            <div class="image-caption">SDEdit with i_start=20</div>
        </div>
        <div>
            <img src="media/145.jpeg" alt="Original Image">
            <div class="image-caption">Original Image</div>
        </div>
    </div>

    <hr class="rounded">

    <div class="image-container">
        <div>
            <img src="media/38.png" alt="SDEdit with i_start=1">
            <div class="image-caption">SDEdit with i_start=1</div>
        </div>
        <div>
            <img src="media/39.png" alt="SDEdit with i_start=3">
            <div class="image-caption">SDEdit with i_start=3</div>
        </div>
        <div>
            <img src="media/40.png" alt="SDEdit with i_start=5">
            <div class="image-caption">SDEdit with i_start=5</div>
        </div>
    </div>
    
    <div class="image-container">
        <div>
            <img src="media/41.png" alt="SDEdit with i_start=7">
            <div class="image-caption">SDEdit with i_start=7</div>
        </div>
        <div>
            <img src="media/42.png" alt="SDEdit with i_start=10">
            <div class="image-caption">SDEdit with i_start=10</div>
        </div>
        <div>
            <img src="media/43.png" alt="SDEdit with i_start=20">
            <div class="image-caption">SDEdit with i_start=20</div>
        </div>
        <div>
            <img src="media/146.jpeg" alt="Original Image">
            <div class="image-caption">Original Image</div>
        </div>
    </div>

    <hr class="rounded">
    <h3> 1.7.1 Editing Hand-Drawn and Web Images </h3>
    <hr class="rounded">

    <p> This procedure works particularly well if we start with a nonrealistic image (e.g. painting, a sketch, some scribbles) and project it onto the natural image manifold. I
        experiment by starting with hand-drawn or other non-realistic images and see how you can get them onto the natural image manifold in fun ways.</p>

    <div class="image-container">
        <div>
            <img src="media/45.png" alt="SDEdit with i_start=1">
            <div class="image-caption">SDEdit with i_start=1</div>
        </div>
        <div>
            <img src="media/46.png" alt="SDEdit with i_start=3">
            <div class="image-caption">SDEdit with i_start=3</div>
        </div>
        <div>
            <img src="media/47.png" alt="SDEdit with i_start=5">
            <div class="image-caption">SDEdit with i_start=5</div>
        </div>
        <div>
            <img src="media/48.png" alt="SDEdit with i_start=7">
            <div class="image-caption">SDEdit with i_start=7</div>
        </div>
    </div>
    
    <div class="image-container">
        <div>
            <img src="media/49.png" alt="SDEdit with i_start=10">
            <div class="image-caption">SDEdit with i_start=10</div>
        </div>
        <div>
            <img src="media/50.png" alt="SDEdit with i_start=20">
            <div class="image-caption">SDEdit with i_start=20</div>
        </div>
        <div>
            <img src="media/44-.png" alt="Web Image">
            <div class="image-caption">Web Image</div>
        </div>
    </div>

    <hr class="rounded">

    <div class="image-container">
        <div>
            <img src="media/52.png" alt="SDEdit with i_start=1">
            <div class="image-caption">SDEdit with i_start=1</div>
        </div>
        <div>
            <img src="media/53.png" alt="SDEdit with i_start=3">
            <div class="image-caption">SDEdit with i_start=3</div>
        </div>
        <div>
            <img src="media/54.png" alt="SDEdit with i_start=5">
            <div class="image-caption">SDEdit with i_start=5</div>
        </div>
        <div>
            <img src="media/55.png" alt="SDEdit with i_start=7">
            <div class="image-caption">SDEdit with i_start=7</div>
        </div>
    </div>
    
    <div class="image-container">
        <div>
            <img src="media/56.png" alt="SDEdit with i_start=10">
            <div class="image-caption">SDEdit with i_start=10</div>
        </div>
        <div>
            <img src="media/57.png" alt="SDEdit with i_start=20">
            <div class="image-caption">SDEdit with i_start=20</div>
        </div>
        <div>
            <img src="media/51.png" alt="Original Sketch">
            <div class="image-caption">Original sketch</div>
        </div>
    </div>

    <hr class="rounded">

    <div class="image-container">
        <div>
            <img src="media/59.png" alt="SDEdit with i_start=1">
            <div class="image-caption">SDEdit with i_start=1</div>
        </div>
        <div>
            <img src="media/60.png" alt="SDEdit with i_start=3">
            <div class="image-caption">SDEdit with i_start=3</div>
        </div>
        <div>
            <img src="media/61.png" alt="SDEdit with i_start=5">
            <div class="image-caption">SDEdit with i_start=5</div>
        </div>
        <div>
            <img src="media/62.png" alt="SDEdit with i_start=7">
            <div class="image-caption">SDEdit with i_start=7</div>
        </div>
    </div>
    
    <div class="image-container">
        <div>
            <img src="media/63.png" alt="SDEdit with i_start=10">
            <div class="image-caption">SDEdit with i_start=10</div>
        </div>
        <div>
            <img src="media/64.png" alt="SDEdit with i_start=20">
            <div class="image-caption">SDEdit with i_start=20</div>
        </div>
        <div>
            <img src="media/58.png" alt="Original sketch">
            <div class="image-caption">Original sketch</div>
        </div>
    </div>

    <hr class="rounded">
    <h3> 1.7.2 Inpainting </h3>
    <hr class="rounded">

    <p> We can use the same procedure to implement inpainting (following the RePaint paper). That is, given an image, x, and a binary mask, m, we can create a new image that has the same 
        content where m is 0, but new content wherever m is 1. To do this, we can run the diffusion denoising loop. But at every step, after obtaining the image, we "force" it to have
        the same pixels as the original images, where m is 0. Essentially, we leave everything inside the edit mask alone, but we replace everything outside the edit mask with our 
        original image - with the correct amount of noise added for timestep, t. </p>

    <div class="image-container">
        <div>
            <img src="media/65.png" alt="Campanile">
            <div class="image-caption">Campanile</div>
        </div>
        <div>
            <img src="media/66.png" alt="Mask">
            <div class="image-caption">Mask</div>
        </div>
        <div>
            <img src="media/67.png" alt="Hole to Fill">
            <div class="image-caption">Hole to Fill</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/68.png" alt="Campanile Inpainted">
            <div class="image-caption">Campanile Inpainted</div>
        </div>
        <div>
            <img src="media/69.png" alt="Campanile Inpainted">
            <div class="image-caption">Campanile Inpainted</div>
        </div>
        <div>
            <img src="media/70.png" alt="Campanile Inpainted">
            <div class="image-caption">Campanile Inpainted</div>
        </div>
        <div>
            <img src="media/71.png" alt="Campanile Inpainted">
            <div class="image-caption">Campanile Inpainted</div>
        </div>
        <div>
            <img src="media/72.png" alt="Campanile Inpainted">
            <div class="image-caption">Campanile Inpainted</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/73.png" alt="London Bridge">
            <div class="image-caption">London Bridge</div>
        </div>
        <div>
            <img src="media/74.png" alt="Mask">
            <div class="image-caption">Mask</div>
        </div>
        <div>
            <img src="media/75.png" alt="Hole to Fill">
            <div class="image-caption">Hole to Fill</div>
        </div>
        <div>
            <img src="media/76.png" alt="London Bridge Inpainted">
            <div class="image-caption">London Bridge Inpainted</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/77.png" alt="Colosseum">
            <div class="image-caption">Colosseum</div>
        </div>
        <div>
            <img src="media/78.png" alt="Mask">
            <div class="image-caption">Mask</div>
        </div>
        <div>
            <img src="media/79.png" alt="Hole to Fill">
            <div class="image-caption">Hole to Fill</div>
        </div>
        <div>
            <img src="media/80.png" alt="Colosseum Inpainted">
            <div class="image-caption">Colosseum Inpainted</div>
        </div>
    </div>

    <hr class="rounded">
    <h3> 1.7.3 Text-Conditional Image-to-image Translation </h3>
    <hr class="rounded">

    <p> Now, we will do the same thing as SDEdit, but guide the projection with a text prompt. This is no longer pure "projection to the natural image manifold" but also adds 
        control using language. </p>

    <div class="image-container">
        <div>
            <img src="media/81.png" alt="Campanile">
            <div class="image-caption">Campanile</div>
        </div>
        <div>
            <img src="media/82.png" alt="Mask">
            <div class="image-caption">Mask</div>
        </div>
        <div>
            <img src="media/83.png" alt="Hole to Fill">
            <div class="image-caption">Hole to Fill</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/84.png" alt="Rocket Ship at noise level 1">
            <div class="image-caption">Rocket Ship at noise level 1</div>
        </div>
        <div>
            <img src="media/85.png" alt="Rocket Ship at noise level 3">
            <div class="image-caption">Rocket Ship at noise level 3</div>
        </div>
        <div>
            <img src="media/86.png" alt="Rocket Ship at noise level 5">
            <div class="image-caption">Rocket Ship at noise level 5</div>
        </div>
    </div>
    
    <div class="image-container">
        <div>
            <img src="media/87.png" alt="Rocket Ship at noise level 7">
            <div class="image-caption">Rocket Ship at noise level 7</div>
        </div>
        <div>
            <img src="media/88.png" alt="Rocket Ship at noise level 10">
            <div class="image-caption">Rocket Ship at noise level 10</div>
        </div>
        <div>
            <img src="media/89.png" alt="Rocket Ship at noise level 20">
            <div class="image-caption">Rocket Ship at noise level 20</div>
        </div>
    </div>

    <hr class="rounded">

    <div class="image-container">
        <div>
            <img src="media/90.png" alt="London Bridge">
            <div class="image-caption">London Bridge</div>
        </div>
        <div>
            <img src="media/91.png" alt="Mask">
            <div class="image-caption">Mask</div>
        </div>
        <div>
            <img src="media/92.png" alt="Hole to Fill">
            <div class="image-caption">Hole to Fill</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/93.png" alt="Man in front of London Bridge at noise level 1">
            <div class="image-caption">Man in front of London Bridge at noise level 1</div>
        </div>
        <div>
            <img src="media/94.png" alt="Man in front of London Bridge at noise level 3">
            <div class="image-caption">Man in front of London Bridge at noise level 3</div>
        </div>
        <div>
            <img src="media/95.png" alt="Man in front of London Bridge at noise level 5">
            <div class="image-caption">Man in front of London Bridge at noise level 5</div>
        </div>
    </div>
    
    <div class="image-container">
        <div>
            <img src="media/96.png" alt="Man in front of London Bridge at noise level 7">
            <div class="image-caption">Man in front of London Bridge at noise level 7</div>
        </div>
        <div>
            <img src="media/97.png" alt="Man in front of London Bridge at noise level 10">
            <div class="image-caption">Man in front of London Bridge at noise level 10</div>
        </div>
        <div>
            <img src="media/98.png" alt="Man in front of London Bridge at noise level 20">
            <div class="image-caption">Man in front of London Bridge at noise level 20</div>
        </div>
    </div>

    <hr class="rounded">

    <div class="image-container">
        <div>
            <img src="media/99.png" alt="Colosseum">
            <div class="image-caption">Colosseum</div>
        </div>
        <div>
            <img src="media/100.png" alt="Mask">
            <div class="image-caption">Mask</div>
        </div>
        <div>
            <img src="media/101.png" alt="Hole to Fill">
            <div class="image-caption">Hole to Fill</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/102.png" alt="Dog in front of Colosseum at noise level 1">
            <div class="image-caption">Dog in front of Colosseum at noise level 1</div>
        </div>
        <div>
            <img src="media/103.png" alt="Dog in front of Colosseum at noise level 3">
            <div class="image-caption">Dog in front of Colosseum at noise level 3</div>
        </div>
        <div>
            <img src="media/104.png" alt="Dog in front of Colosseum at noise level 5">
            <div class="image-caption">Dog in front of Colosseum at noise level 5</div>
        </div>
    </div>
    
    <div class="image-container">
        <div>
            <img src="media/105.png" alt="Dog in front of Colosseum at noise level 7">
            <div class="image-caption">Dog in front of Colosseum at noise level 7</div>
        </div>
        <div>
            <img src="media/106.png" alt="Dog in front of Colosseum at noise level 10">
            <div class="image-caption">Dog in front of Colosseum at noise level 10</div>
        </div>
        <div>
            <img src="media/107.png" alt="Dog in front of Colosseum at noise level 20">
            <div class="image-caption">Dog in front of Colosseum at noise level 20</div>
        </div>
    </div>

    <hr class="rounded">
    <h3> 1.8 Visual Anagrams </h3>
    <hr class="rounded">

    <p> In this part, we are finally ready to implement Visual Anagrams and create optical illusions with diffusion models. In this part, I first create an image that looks like
        "an oil painting of people around a campfire", but when flipped upside down will reveal "an oil painting of an old man". To do this, we will denoise an image, x, at step, t,
        normally with the prompt "an oil painting of an old man", to obtain noise estimate e1. But at the same time, we will flip x upside down, and denoise with the prompt 
        "an oil painting of people around a campfire", to get noise estimate e2. We can flip e2 back, to make it right-side up, and average the two noise estimates to get the final 
        estimate. We can then perform a reverse/denoising diffusion step with the averaged noise estimate. </p>

    <div class="image-container">
        <div>
            <img src="media/108.png" alt="An Oil Painting of People around a Campfire">
            <div class="image-caption">An Oil Painting of People around a Campfire</div>
        </div>
        <div>
            <img src="media/109.png" alt="An Oil Painting of an Old Man">
            <div class="image-caption">An Oil Painting of an Old Man</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/110.png" alt="An oil painting of a snowy mountain village">
            <div class="image-caption">An oil painting of a snowy mountain village</div>
        </div>
        <div>
            <img src="media/111.png" alt="A photo of the amalfi coast">
            <div class="image-caption">A photo of the amalfi coast</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/112.png" alt="An oil painting of a snowy mountain village">
            <div class="image-caption">An oil painting of a snowy mountain village</div>
        </div>
        <div>
            <img src="media/113.png" alt="An oil painting of people around a campfire">
            <div class="image-caption">An oil painting of people around a campfire</div>
        </div>
    </div>

    <hr class="rounded">
    <h3> 1.9 Hybrid Images </h3>
    <hr class="rounded">

    <p> In this part we'll implement Factorized Diffusion and create hybrid images just like in project 2. In order to create hybrid images with a diffusion model we can use a 
        similar technique as above. We will create a composite noise estimate by estimating the noise with two different text prompts, and then combining low frequencies from 
        one noise estimate with high frequencies of the other.</p>

    <div class="image-container">
        <div>
            <img src="media/114.png" alt="Hybrid image of a skull and a waterfall">
            <div class="image-caption">Hybrid image of a skull and a waterfall</div>
        </div>
        <div>
            <img src="media/115.png" alt="Hybrid image of a skull and the amalfi coast">
            <div class="image-caption">Hybrid image of a skull and the amalfi coast</div>
        </div>
        <div>
            <img src="media/116.png" alt="Hybrid image of a dog and people around a campfire">
            <div class="image-caption">Hybrid image of a dog and people around a campfire</div>
        </div>
    </div>

    <hr class="rounded">
    <h2> Part B: Diffusion Models from scratch! </h2>
    <hr class="rounded">

    <hr class="rounded">
    <h3> Part 1: Training a Single-Step Denoising UNet </h3>
    <hr class="rounded">

    <p> I used the below diagram to construct the various classes that I need to create the UNet: </p>

    <div class="image-container">
        <div>
            <img src="media/130.png" alt="Unconditional UNet">
            <div class="image-caption">Unconditional UNet</div>
        </div>
    </div>

    <p> I used the below diagram to construct the UnconditionalUNet Class: </p>

    <div class="image-container">
        <div>
            <img src="media/130.png" alt="Standard UNet Operations">
            <div class="image-caption">Standard UNet Operations</div>
        </div>
    </div>

    <p> Now, we will train the model to perform denoising. The objective is to train a denoiser to denoise a noisy image, which is essentially the product of a noise level of 0.5 
        applied to a clean image. I use the MNIST dataset via torchvision.datasets.MNIST with flags to access both the training and test sets. Then, I train only on the training 
        set over 5 epochs. Before I create the dataloader, I shuffle the dataset and use the recommended batch size of 256. I only noise the image batches when fetched from the 
        dataloader so that in every epoch the network will see new noised images thus improving generalization. For the model, I use the UNet architecture previously defined 
        with recommended hidden dimension D = 128. For the optimizer, I use the Adam optimizer with learning rate of 1e-4.</p>

    <div class="image-container">
        <div>
            <img src="media/117.png" alt="Varying levels of noise on MNIST digits">
            <div class="image-caption">Varying levels of noise on MNIST digits</div>
        </div>
        <div>
            <img src="media/131.png" alt="Results on digits from the test set after 1 epoch of training">
            <div class="image-caption">Results on digits from the test set after 1 epoch of training</div>
        </div>
        <div>
            <img src="media/132.png" alt="Results on digits from the test set after 5 epochs of training">
            <div class="image-caption">Results on digits from the test set after 5 epochs of training</div>
        </div>
    </div>

    <p> Our denoiser was trained on MNIST digits noised with noise level of 0.5. Now, let's see how the denoiser performs on different noise levels that it wasn't trained for. 
        I visualized the denoiser results on test set digits with varying levels of noise = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]. </p>

    <div class="image-container">
        <div>
            <img src="media/133.png" alt="Results on digits from the test set with varying noise levels">
            <div class="image-caption">Results on digits from the test set with varying noise levels</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/121.png" alt="Training Loss Curve">
            <div class="image-caption">Training Loss Curve</div>
        </div>
    </div>

    <hr class="rounded">
    <h3> Part 2: Training a Diffusion Model </h3>
    <hr class="rounded">

    <hr class="rounded">
    <h3> Time-Conditioned UNet </h3>
    <hr class="rounded">

    <p> We need a way to inject scalar t into our UNet model to condition it. There are many ways to do this but this is how I implemented it: </p>

    <div class="image-container">
        <div>
            <img src="media/137.png" alt="Time-Conditioned UNet">
            <div class="image-caption">Time-Conditioned UNet</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/138.png" alt="FCBlock for conditioning">
            <div class="image-caption">FCBlock for conditioning</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/139.png" alt="Training time-conditioned UNet">
            <div class="image-caption">Training time-conditioned UNet</div>
        </div>
        <div>
            <img src="media/140.png" alt="Sampling from time-conditioned UNet">
            <div class="image-caption">Sampling from time-conditioned UNet</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/143.png" alt="Epoch 5">
            <div class="image-caption">Epoch 5</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/144.png" alt="Epoch 20">
            <div class="image-caption">Epoch 20</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/124.png" alt="Time-Conditioned UNet training loss curve">
            <div class="image-caption">Time-Conditioned UNet training loss curve</div>
        </div>
    </div>

    <hr class="rounded">
    <h3> Class-Conditioned UNET </h3>
    <hr class="rounded">

    <p> To make the results better and give us more control for image generation, we can also condition our UNet on the class of the digit 0-9. This will require 
        adding 2 more FCBlocks to our UNet but for the class-conditioning vector, you make it a one-hot vector instead of a single scalar. Since we still want our UNet 
        to work without it being conditioned on the class, we implement dropout where 10% of the time, we drop the class conditioning vector by setting it to 0. Training for 
        this section will be the same as time-only, with the only difference being the conditioning vector and doing unconditional generation periodically. The sampling process 
        is the same as part A, where we saw that conditional results aren't good unless we use classifier-free guidance so I used CFG with gamma = 5.0 for this part.</p>

    <div class="image-container">
        <div>
            <img src="media/141.png" alt="Training class-conditioned UNet">
            <div class="image-caption">Training class-conditioned UNet</div>
        </div>
        <div>
            <img src="media/142.png" alt="Sampling from class-conditioned UNet">
            <div class="image-caption">Sampling from class-conditioned UNet</div>
        </div>
    </div>
    
    <div class="image-container">
        <div>
            <img src="media/125.png" alt="Epoch 5">
            <div class="image-caption">Epoch 5</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/126.png" alt="Epoch 20">
            <div class="image-caption">Epoch 20</div>
        </div>
    </div>

    <div class="image-container">
        <div>
            <img src="media/128.png" alt="Class-Conditioned UNet training loss curve">
            <div class="image-caption">Class-Conditioned UNet training loss curve</div>
        </div>
    </div>
    
</body>
</html>
